{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑斯蒂回归分类算法的信用卡欺诈检测(01变量）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "商业价值：\n",
    "1. 金融领域：对互联网金融产品进行用户分析，同时可以监测商业欺诈\n",
    "2. 电商领域：对客户进行精确分类，从而实现精准营销\n",
    "3. 医疗领域：对疾病诊断结果进行快速检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.读取数据，分析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(os.getcwd()).parent # 获取.ipynb当前目录上1层目录位置\n",
    "input_file1=PROJECT_ROOT /\"data\"/\"data_creditcard.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>时间</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>交易金额</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>172783.0</td>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>172784.0</td>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>172785.0</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284797 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              时间        V1        V2        V3        V4        V5        V6  \\\n",
       "0            2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "1            4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "2            7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "3            7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "4            9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284792  172782.0 -0.241923  0.712247  0.399806 -0.463406  0.244531 -1.343668   \n",
       "284793  172782.0  0.219529  0.881246 -0.635891  0.960928 -0.152971 -1.014307   \n",
       "284794  172783.0 -1.775135 -0.004235  1.189786  0.331096  1.196063  5.519980   \n",
       "284795  172784.0  2.039560 -0.175233 -1.196825  0.234580 -0.008713 -0.726571   \n",
       "284796  172785.0  0.120316  0.931005 -0.546012 -0.745097  1.130314 -0.235973   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398   \n",
       "1      -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
       "2       1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
       "3       0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
       "4       0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284792  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376  0.279598   \n",
       "284793  0.427126  0.121340 -0.285670  ...  0.099936  0.337120  0.251791   \n",
       "284794 -1.518185  2.080825  1.159498  ...  0.103302  0.654850 -0.348929   \n",
       "284795  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211  0.297930   \n",
       "284796  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520  0.050343   \n",
       "\n",
       "             V24       V25       V26       V27       V28   交易金额  类别  \n",
       "0      -0.371427 -0.232794  0.105915  0.253844  0.081080   3.67   0  \n",
       "1      -0.780055  0.750137 -0.257237  0.034507  0.005168   4.99   0  \n",
       "2      -0.649709 -0.415267 -0.051634 -1.206921 -1.085339  40.80   0  \n",
       "3       1.011592  0.373205 -0.384157  0.011747  0.142404  93.20   0  \n",
       "4      -0.385050 -0.069733  0.094199  0.246219  0.083076   3.68   0  \n",
       "...          ...       ...       ...       ...       ...    ...  ..  \n",
       "284792  0.371441 -0.559238  0.113144  0.131507  0.081265   5.49   0  \n",
       "284793  0.057688 -1.508368  0.144023  0.181205  0.215243  24.05   0  \n",
       "284794  0.745323  0.704545 -0.127579  0.454379  0.130308  79.99   0  \n",
       "284795 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   2.68   0  \n",
       "284796  0.102800 -0.435870  0.124079  0.217940  0.068803   2.69   0  \n",
       "\n",
       "[284797 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(input_file1,encoding='gbk')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       492\n",
       "0    284305\n",
       "Name: 类别, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.value_counts:Return a Series containing the frequency of each distinct row in the Dataframe（检查数据均衡）\n",
    "count_classes = data['类别'].value_counts(sort=True, # Sort by frequencies \n",
    "                                       ascending =True,\n",
    "                                       dropna=True) # 空值不计数\n",
    "count_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分类列‘类别’数据不均衡 1.下采样 2.过采样 SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.0 交易金额处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>时间</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>交易金额</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>172783.0</td>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>172784.0</td>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>172785.0</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265855 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              时间        V1        V2        V3        V4        V5        V6  \\\n",
       "0            2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "1            4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "2            7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "3            7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "4            9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284792  172782.0 -0.241923  0.712247  0.399806 -0.463406  0.244531 -1.343668   \n",
       "284793  172782.0  0.219529  0.881246 -0.635891  0.960928 -0.152971 -1.014307   \n",
       "284794  172783.0 -1.775135 -0.004235  1.189786  0.331096  1.196063  5.519980   \n",
       "284795  172784.0  2.039560 -0.175233 -1.196825  0.234580 -0.008713 -0.726571   \n",
       "284796  172785.0  0.120316  0.931005 -0.546012 -0.745097  1.130314 -0.235973   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398   \n",
       "1      -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
       "2       1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
       "3       0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
       "4       0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284792  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376  0.279598   \n",
       "284793  0.427126  0.121340 -0.285670  ...  0.099936  0.337120  0.251791   \n",
       "284794 -1.518185  2.080825  1.159498  ...  0.103302  0.654850 -0.348929   \n",
       "284795  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211  0.297930   \n",
       "284796  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520  0.050343   \n",
       "\n",
       "             V24       V25       V26       V27       V28   交易金额  类别  \n",
       "0      -0.371427 -0.232794  0.105915  0.253844  0.081080   3.67   0  \n",
       "1      -0.780055  0.750137 -0.257237  0.034507  0.005168   4.99   0  \n",
       "2      -0.649709 -0.415267 -0.051634 -1.206921 -1.085339  40.80   0  \n",
       "3       1.011592  0.373205 -0.384157  0.011747  0.142404  93.20   0  \n",
       "4      -0.385050 -0.069733  0.094199  0.246219  0.083076   3.68   0  \n",
       "...          ...       ...       ...       ...       ...    ...  ..  \n",
       "284792  0.371441 -0.559238  0.113144  0.131507  0.081265   5.49   0  \n",
       "284793  0.057688 -1.508368  0.144023  0.181205  0.215243  24.05   0  \n",
       "284794  0.745323  0.704545 -0.127579  0.454379  0.130308  79.99   0  \n",
       "284795 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   2.68   0  \n",
       "284796  0.102800 -0.435870  0.124079  0.217940  0.068803   2.69   0  \n",
       "\n",
       "[265855 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 空值\n",
    "data['交易金额'].isna().sum() \n",
    "# 异常值\n",
    "q1,q3 = np.quantile(data['交易金额'],[0.25,0.75])\n",
    "iqr = q3 - q1 \n",
    "mask_high = data['交易金额'] >= q3 + iqr*3\n",
    "mask_low = data['交易金额'] <= q1 - iqr*3\n",
    "data = data.loc[~mask_high]\n",
    "data = data.loc[~mask_low]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>时间</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>交易金额</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>172783.0</td>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>172784.0</td>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>172785.0</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264823 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              时间        V1        V2        V3        V4        V5        V6  \\\n",
       "0            2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "1            4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "2            7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "3            7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "4            9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284792  172782.0 -0.241923  0.712247  0.399806 -0.463406  0.244531 -1.343668   \n",
       "284793  172782.0  0.219529  0.881246 -0.635891  0.960928 -0.152971 -1.014307   \n",
       "284794  172783.0 -1.775135 -0.004235  1.189786  0.331096  1.196063  5.519980   \n",
       "284795  172784.0  2.039560 -0.175233 -1.196825  0.234580 -0.008713 -0.726571   \n",
       "284796  172785.0  0.120316  0.931005 -0.546012 -0.745097  1.130314 -0.235973   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398   \n",
       "1      -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
       "2       1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
       "3       0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
       "4       0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284792  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376  0.279598   \n",
       "284793  0.427126  0.121340 -0.285670  ...  0.099936  0.337120  0.251791   \n",
       "284794 -1.518185  2.080825  1.159498  ...  0.103302  0.654850 -0.348929   \n",
       "284795  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211  0.297930   \n",
       "284796  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520  0.050343   \n",
       "\n",
       "             V24       V25       V26       V27       V28   交易金额  类别  \n",
       "0      -0.371427 -0.232794  0.105915  0.253844  0.081080   3.67   0  \n",
       "1      -0.780055  0.750137 -0.257237  0.034507  0.005168   4.99   0  \n",
       "2      -0.649709 -0.415267 -0.051634 -1.206921 -1.085339  40.80   0  \n",
       "3       1.011592  0.373205 -0.384157  0.011747  0.142404  93.20   0  \n",
       "4      -0.385050 -0.069733  0.094199  0.246219  0.083076   3.68   0  \n",
       "...          ...       ...       ...       ...       ...    ...  ..  \n",
       "284792  0.371441 -0.559238  0.113144  0.131507  0.081265   5.49   0  \n",
       "284793  0.057688 -1.508368  0.144023  0.181205  0.215243  24.05   0  \n",
       "284794  0.745323  0.704545 -0.127579  0.454379  0.130308  79.99   0  \n",
       "284795 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   2.68   0  \n",
       "284796  0.102800 -0.435870  0.124079  0.217940  0.068803   2.69   0  \n",
       "\n",
       "[264823 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移除所有重复值\n",
    "data.drop_duplicates(inplace = True) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 标准化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‘交易金额’列，数据波动大，需要标准化 StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# StandardScaler(‘先学fit后transform’的特征转换） mean：0 divination：1 / 数据泄露 \n",
    "# 输入要求 array ->values,且二维 -> reshape\n",
    "data['标准化交易金额'] = StandardScaler().fit_transform(data['交易金额'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 去掉多余列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>类别</th>\n",
       "      <th>标准化交易金额</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.667618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.820260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>-0.284708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.659184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>-0.111640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.346124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>-0.594242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>0</td>\n",
       "      <td>0.597441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>0.267772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.706581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>-0.657422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.706413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264823 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "1       1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "2      -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "3      -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "4      -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284792 -0.241923  0.712247  0.399806 -0.463406  0.244531 -1.343668  0.929369   \n",
       "284793  0.219529  0.881246 -0.635891  0.960928 -0.152971 -1.014307  0.427126   \n",
       "284794 -1.775135 -0.004235  1.189786  0.331096  1.196063  5.519980 -1.518185   \n",
       "284795  2.039560 -0.175233 -1.196825  0.234580 -0.008713 -0.726571  0.017050   \n",
       "284796  0.120316  0.931005 -0.546012 -0.745097  1.130314 -0.235973  0.812722   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "0       0.260314 -0.568671 -0.371407  ... -0.208254 -0.559825 -0.026398   \n",
       "1       0.081213  0.464960 -0.099254  ... -0.167716 -0.270710 -0.154104   \n",
       "2      -3.807864  0.615375  1.249376  ...  1.943465 -1.015455  0.057504   \n",
       "3       0.851084 -0.392048 -0.410430  ... -0.073425 -0.268092 -0.204233   \n",
       "4       0.069539 -0.736727 -0.366846  ... -0.246914 -0.633753 -0.120794   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284792 -0.206210  0.106234 -0.284708  ... -0.228876 -0.514376  0.279598   \n",
       "284793  0.121340 -0.285670 -0.111640  ...  0.099936  0.337120  0.251791   \n",
       "284794  2.080825  1.159498 -0.594242  ...  0.103302  0.654850 -0.348929   \n",
       "284795 -0.118228  0.435402  0.267772  ... -0.268048 -0.717211  0.297930   \n",
       "284796  0.115093 -0.204064 -0.657422  ... -0.314205 -0.808520  0.050343   \n",
       "\n",
       "             V24       V25       V26       V27       V28  类别   标准化交易金额  \n",
       "0      -0.371427 -0.232794  0.105915  0.253844  0.081080   0 -0.689883  \n",
       "1      -0.780055  0.750137 -0.257237  0.034507  0.005168   0 -0.667618  \n",
       "2      -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   0 -0.063594  \n",
       "3       1.011592  0.373205 -0.384157  0.011747  0.142404   0  0.820260  \n",
       "4      -0.385050 -0.069733  0.094199  0.246219  0.083076   0 -0.689714  \n",
       "...          ...       ...       ...       ...       ...  ..       ...  \n",
       "284792  0.371441 -0.559238  0.113144  0.131507  0.081265   0 -0.659184  \n",
       "284793  0.057688 -1.508368  0.144023  0.181205  0.215243   0 -0.346124  \n",
       "284794  0.745323  0.704545 -0.127579  0.454379  0.130308   0  0.597441  \n",
       "284795 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   0 -0.706581  \n",
       "284796  0.102800 -0.435870  0.124079  0.217940  0.068803   0 -0.706413  \n",
       "\n",
       "[264823 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去掉多余列\n",
    "# 逻辑斯蒂回归自动识别DF中的“除标签列以外”的其他所有列 as an individual feature(The entire input X is my matrix of features.)\n",
    "data = data.drop(['时间','交易金额'],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.用下采样方案训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 先划分训练集、测试集，再在训练集里完成下采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保留异常数据，从正常数据里，随机选取和异常数据相同数量的样本（利用索引操作）np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原始倾斜数据 训练集包含样本数量:  185376\n",
      "原始倾斜数据 测试集包含样本数量:  79447\n",
      "原始倾斜数据 样本总数:  264823\n"
     ]
    }
   ],
   "source": [
    "# 取整体数据的特征列、标签列(分类列）\n",
    "X = data.loc[:, data.columns != '类别'] # 处标签列外，其他特征列均考虑\n",
    "y = data.loc[:, data.columns == '类别']\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "# 对原始倾斜数据集进行切分（注意选择相同的随机策略）\n",
    "# 在 X_train 和 y_train集里面进行下采用，不要污染test集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\n",
    "print(\"\")\n",
    "print(\"原始倾斜数据 训练集包含样本数量: \", len(X_train))\n",
    "print(\"原始倾斜数据 测试集包含样本数量: \", len(X_test))\n",
    "print(\"原始倾斜数据 样本总数: \", len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/indexing.py:1587\u001B[0m, in \u001B[0;36m_iLocIndexer._get_list_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1587\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_with_is_copy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1589\u001B[0m     \u001B[38;5;66;03m# re-raise with different error message\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame._take_with_is_copy\u001B[0;34m(self, indices, axis)\u001B[0m\n\u001B[1;32m   3895\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3896\u001B[0m \u001B[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001B[39;00m\n\u001B[1;32m   3897\u001B[0m \u001B[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3900\u001B[0m \u001B[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001B[39;00m\n\u001B[1;32m   3901\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m-> 3902\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3903\u001B[0m \u001B[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/generic.py:3886\u001B[0m, in \u001B[0;36mNDFrame._take\u001B[0;34m(self, indices, axis, convert_indices)\u001B[0m\n\u001B[1;32m   3884\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n\u001B[0;32m-> 3886\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3888\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_block_manager_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3889\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3891\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3892\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_data)\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/internals/managers.py:975\u001B[0m, in \u001B[0;36mBaseBlockManager.take\u001B[0;34m(self, indexer, axis, verify, convert_indices)\u001B[0m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_indices:\n\u001B[0;32m--> 975\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[43mmaybe_convert_indices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    977\u001B[0m new_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes[axis]\u001B[38;5;241m.\u001B[39mtake(indexer)\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/indexers/utils.py:286\u001B[0m, in \u001B[0;36mmaybe_convert_indices\u001B[0;34m(indices, n, verify)\u001B[0m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m--> 286\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindices are out-of-bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m indices\n",
      "\u001B[0;31mIndexError\u001B[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m under_sample_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([fraud_indices,random_normal_indices])\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 根据索引，获取下采样数据集，注意这里一直是原始数据集的索引，需要在原始数据里面检索\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m under_sample_data \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43munder_sample_indices\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# 切分出下采样样本的特征、标签(二分类列）\u001B[39;00m\n\u001B[1;32m     20\u001B[0m X_undersample \u001B[38;5;241m=\u001B[39m under_sample_data\u001B[38;5;241m.\u001B[39mloc[:, under_sample_data\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m类别\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;66;03m# ‘类别’外所有列为特征列（用 loc + Boolean array可定位)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/indexing.py:1073\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1070\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1072\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m-> 1073\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/indexing.py:1616\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1614\u001B[0m \u001B[38;5;66;03m# a list of integers\u001B[39;00m\n\u001B[1;32m   1615\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like_indexer(key):\n\u001B[0;32m-> 1616\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_list_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1618\u001B[0m \u001B[38;5;66;03m# a single integer\u001B[39;00m\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1620\u001B[0m     key \u001B[38;5;241m=\u001B[39m item_from_zerodim(key)\n",
      "File \u001B[0;32m~/anaconda0412/anaconda3/envs/ml_env/lib/python3.9/site-packages/pandas/core/indexing.py:1590\u001B[0m, in \u001B[0;36m_iLocIndexer._get_list_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1587\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_take_with_is_copy(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1589\u001B[0m     \u001B[38;5;66;03m# re-raise with different error message\u001B[39;00m\n\u001B[0;32m-> 1590\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositional indexers are out-of-bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# 获取异常值的数量、索引号\n",
    "rng = np.random.RandomState(42) # 使用 RandomState 对象 为choice设置随机数\n",
    "\n",
    "number_records_fraud = len(y_train[y_train.类别 == 1])\n",
    "fraud_indices = np.array(y_train[y_train.类别 == 1].index)\n",
    "\n",
    "# 从正常样本的索引中随机选择和异常样本数量相同的索引，转为array\n",
    "normal_indices = y_train[y_train.类别 == 0].index \n",
    "random_normal_indices = rng.choice(normal_indices, # What to choose from\n",
    "                                         size = number_records_fraud, #  returns array of * elements \n",
    "                                         replace = False) # 相同数据不可重复被选择random sample    默认是True：same item can be chosen multiple times                                                            \n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# 将异常样本索引和随机选择的正常样本的索引拼起来，成为下采样样本索引\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "# 根据索引，获取下采样数据集，注意这里一直是原始数据集的索引，需要在原始数据里面检索\n",
    "under_sample_data = data.iloc[under_sample_indices]\n",
    "\n",
    "# 切分出下采样样本的特征、标签(二分类列）\n",
    "X_undersample = under_sample_data.loc[:, under_sample_data.columns != '类别'] # ‘类别’外所有列为特征列（用 loc + Boolean array可定位)\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == '类别'] # ‘类别’为标签列\n",
    "\n",
    "print(\"下采样样本内，正常样本占比: \", len(under_sample_data[under_sample_data.类别 == 0])/len(under_sample_data))\n",
    "print(\"下采样样本内，异常样本占比: \", len(under_sample_data[under_sample_data.类别 == 1])/len(under_sample_data))\n",
    "print(\"下采样策略总体样本数量: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对下采样数据集进行切分\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 42)\n",
    "# test_size: 0.3代表测试集占30% ，random_state:always use 随机数生成器种子\n",
    "print(\"下采样 训练集包含样本数量: \", len(X_train_undersample))\n",
    "print(\"下采样 测试集包含样本数量: \", len(X_test_undersample))\n",
    "print(\"下采样策略总体样本数量:\", len(X_train_undersample)+len(X_test_undersample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集切分，数据集切分为训练集和测试集 train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix,recall_score # 混淆矩阵、召回率（评估二分类回归的标准）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归 LogisticRegression(C = c_param, penalty = 'l1', solver='liblinear') \n",
    "\n",
    "```1.模型参数确认 C ：KFold、cross_val_score 交叉验证 / 最优GridSearchCV + KFold ```\n",
    "\n",
    "The model is trained on K-1 folds and tested on the remaining 1 fold.\n",
    "This process repeats K times with each fold serving as “the test set” once.\n",
    "\n",
    "```2.检验该模型的标准：混淆矩阵、召回率```\n",
    "\n",
    "recall_score = TP/(TP+FN) 预测的召回率（越高越好）：真正异常数据中，有多少被正确预测。\n",
    "  \n",
    "“异常未被捕获错误”:FN 被错误分类的负例 ｜ “浪费资源的错误”:FP 被错误分类的正例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动循环\n",
    "def printing_Kfold_scores(x_train_data,y_train_data): \n",
    "    fold = KFold(n_splits=5,shuffle=False) # 创建 KFold对象（K折交叉验证 K equal parts） \n",
    "    \n",
    "    # 定义不同的正则强度\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "    \n",
    "    # 展示结果表 5行2列 每个强度对应召回系数的均值\n",
    "    results_table = pd.DataFrame(columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "    \n",
    "    j = 0 #  外层 遍历不同的正则化强度（**j用于全局表格上索引，i是内部索引，不能混，需要区分）\n",
    "    for c_param in c_param_range: \n",
    "        print('-------------------------------------------')\n",
    "        print('正则化强度: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "        \n",
    "        # 内层 每个fold的召回率\n",
    "        recall_accs = []\n",
    "        for iteration, indices in enumerate(fold.split(x_train_data),start=1): \n",
    "            # iteration ：i值，第i次交叉验证 ； \n",
    "            # kfold.split(x_train_data)是个generator,返回2个索引切片 indices (array positions)，train_index 和 test_index\n",
    "            # 训练集的索引 indices[0] ，测试集的索引 indices[1] ，利用索引操作比数据本身要好\n",
    "            \n",
    "            # C 外层遍历的正则化强度 、L1 正则化、优化算法 liblinear 适合小数据集且L1的情况、random_state(liblinear变）\n",
    "            # estimator先fit(x，y)学习，y to be a 1D array. 即使1列的df,.values后是2D(n_sample,1),ravel()压成1D；\n",
    "            # x 不能.values ,转换成array后会丢失features name，但模型需要features name \n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1', solver='liblinear',random_state=42,max_iter=12000) \n",
    "            \n",
    "            # 训练模型：训练集索引indices[0]，获取训练集数据-自变量、因变量\n",
    "            lr.fit(x_train_data.iloc[indices[0]],y_train_data.iloc[indices[0]].values.ravel()) \n",
    "            # 模型预测：测试集索引 indices[1]，测试集数据-自变量\n",
    "            y_pred_data = lr.predict(x_train_data.iloc[indices[1]])\n",
    "            \n",
    "            # recall_score(y_actural, y_prediction) 需要“相同数据集”的y_actural 和 y_prediction 做参数。\n",
    "            # 在y_train_data上，用相同索引获得了“测试集数据的真实y值”\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1]], y_pred_data)\n",
    "            recall_accs.append(recall_acc) # 5个fold放进list,np.mean()即可求平均\n",
    "            print('Iteration ', iteration,': 召回率= ', recall_acc)\n",
    "        \n",
    "        # 执行完所有的交叉验证后，每个参数 c_param 对应的召回率平均值\n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('平均召回率: ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].astype(float).idxmax()]['C_parameter']\n",
    "    \n",
    "    print('*********************************************************************************')\n",
    "    print('效果最好的模型所选参数 = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample) # 传入下采样训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵（cm:计算出的混淆矩阵的值，classes：分类标签，cmap:绘图样式\n",
    "def plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm,cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar() # 灰度条\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用下采样数据的测试集，测试参数确定后的下采样方案模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1',solver='liblinear',random_state=42,max_iter=12000)\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample)\n",
    "\n",
    "# 计算混淆矩阵（相同测试集上的真实值、预测值） a 1D list, NumPy array, or pandas Series. [0, 1, 1, 0, 1]\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# 有了混淆矩阵的情况下，计算TP/(FN+TP) 。FN 真实值=1，预测值=0 先y后x\n",
    "print(\"测试集中的召回率(下采样）: \",cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果解释：这是理想情况下的模型测试结果，因为下采样数据集中，异常样本：正常样本=1:1，原始数据的实际情况是 28W：500。\n",
    "\n",
    "用原始数据的测试集，测试“下采样方法”训练出的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_undersample_model = lr.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred_undersample_model)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"测试集中的召回率（原始数据）: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果解释：这是实际情况下的测试，可以看出--下采样数据集上训练处的模型，应用在原始数据量大的数据上，召回率偏差不大。\n",
    "\n",
    "但实际为正常数据而被预测为异常数据的数据量占比偏高，假阳性变多。FP（误报）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.下采样方案训练模型改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 不采样方案直接训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一开始就用倾斜数据进行模型的训练，而不采用改进的“下采样方法”训练模型，结果会怎样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过混淆矩阵来查看原始数据训练模型的测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = best_c, penalty = 'l1',solver='liblinear',random_state=42,max_iter=12000)\n",
    "lr.fit(X_train,y_train.values.ravel())\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"测试集中的召回率: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果解释：如果使用原始数据训练模型，测试结果-- FP ：假阳性变少，但相对应的召回率较低，很多异常数据没有发现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 调整下采样方案阈值并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原先下采样默认阈值0.5，现在指定阈值 ls.predict_proba()，评估下采样方案各阈值下的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 0.01, penalty = 'l1',solver='liblinear',random_state=42,max_iter=12000) # 下采样数据得到的最优C\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())# 用下采样数据训练模型\n",
    "# 样本归属各类01的概率值 a sample belongs to each class. a 2-dimensional NumPy array  shape(n_samples, n_classes) n_classes二分类=2\n",
    "# 结果每个样本 [0.89  0.11] 归属类的概率\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample) \n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "j = 1\n",
    "for i in thresholds:\n",
    "    # 布尔过滤 : 归属为异常值的概率大于该阈值 的数据.\n",
    "    # 假设i=0.1 ,代表只要样本的异常值概率大于0.1，就会被筛选出标记为 True，true=1 \n",
    "    # 混淆矩阵时，异常判断过于宽松，很多被算为异常值。recall=1 也就是代表，所有都被算作异常值。\n",
    "    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j) # 3*3子图，J是第几个图，每张图对应一个阈值\n",
    "    j += 1 # 设置子图起点\n",
    "    \n",
    "    # a 1D list, NumPy array, or pandas Series. [0, 1, 1, 0, 1]\n",
    "    cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall) # （真实，预测）\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print(f\"阈值={i}时，测试集中的召回率: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "    class_names = [0,1]\n",
    "    plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Threshold >= %s'%i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果解释：\n",
    "\n",
    "1.随着阈值变大，异常值判断标准提高，检出率下降；2.当阈值为0.1-0.4时，召回率为1，所有人都被预测为正例（异常）没有意义，阈值过于宽松；\n",
    "2.当阈值为0.5时，误报人数下降，但小比较0.6时，0.6的误报人数下降明显，模型正确检出的负例也有所增加。\n",
    "3.0.6比0.7，0.6的召回率高（检出异常率高），只是误报的人数相较0.7要高。考虑召回率，优先使用0.6作为阈值，考虑误报人数，优先使用0.7作为阈值；\n",
    "5.阈值0.8-0.9 召回率低不考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 用新阈值再次预测原始数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择阈值是0.6和0.7的下采样模型，使用原始数据的测试集，预测效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_undersample_proba = lr.predict_proba(X_test) \n",
    "y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > 0.7\n",
    "    \n",
    "cnf_matrix = confusion_matrix(y_test,y_test_predictions_high_recall) # （真实，预测）\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"测试集中的召回率（原始数据）: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "class_names = [0,1]\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Threshold >= %s'%0.7) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 过采样SMOTE方案训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from imblearn.over_sampling import SMOTE # Imbalanced-Learn 库 \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 建立过采样训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_cards = data\n",
    "credit_cards.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据，划分特征和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=credit_cards['类别']\n",
    "features=credit_cards.loc[:,credit_cards.columns != '类别']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, \n",
    "                                                                            labels, \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 SMOTE过采样样本、确定模型最优参数C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集中进行过采样样本 (SMOTE 自动识别少数类，不需要手动）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler=SMOTE(random_state=42)\n",
    "os_features,os_labels=oversampler.fit_resample(features_train,labels_train)\n",
    "\n",
    "print(f\"原始训练集类别分布:{pd.Series(labels_train).value_counts()}\")\n",
    "print(f\"SMOTE后训练集类别分布:{pd.Series(os_labels).value_counts()}\")\n",
    "print(f\"原始数据形状: {features_train.shape}\")\n",
    "print(f\"过采样后数据形状: {os_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取最优参数C的值 best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_features = pd.DataFrame(os_features)\n",
    "os_labels = pd.DataFrame(os_labels)\n",
    "best_c = printing_Kfold_scores(os_features,os_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "使用平衡后的训练集训练模型,使用原始不平衡测试集去预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 训练过采样模型，测试原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = best_c, penalty = 'l1',solver='liblinear',random_state=42,max_iter=12000)\n",
    "lr.fit(os_features,os_labels.values.ravel()) # 过采样后数据训练模型\n",
    "\n",
    "y_pred = lr.predict(features_test) # 原始数据的测试集预测\n",
    "\n",
    "cnf_matrix = confusion_matrix(labels_test,y_pred) # （原始数据测试集的label真实值 ， 预测值）\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"测试集中的召回率: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()\n",
    "print(f\"特征值的系数列表：{lr.coef_}\")\n",
    "print(f\"截距项：{lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "上述结论：采用0.7阈值训练出来的“下采样模型”和使用“过采样SMOTE”训练出的模型相比，\n",
    "在原始测试集数据上的表现要好（召回率类似、误报率低）。优先使用带阈值的下采样模型。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
